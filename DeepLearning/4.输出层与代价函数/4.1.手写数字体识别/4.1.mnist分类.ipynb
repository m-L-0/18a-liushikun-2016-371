{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 下载MNIST数据集并生成DataSet对象\n",
    "# 使用OneHot编码处理标记\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集图片矩阵，代表55000张图片，每张图片为一个向量，其长度为784\n",
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集标记矩阵，代表55000张图片的标记，每张图片为一个10维的独热编码向量\n",
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee804d9d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化训练集中的图片\n",
    "plt.imshow(Image.fromarray(mnist.train.images[0].reshape(28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee805676a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 可视化训练集中的图片\n",
    "plt.imshow(Image.fromarray((mnist.train.images[1] * 255).astype(np.uint8).reshape([28,28])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 15.6144, acc 0.0694\n",
      "step   500, loss 13.4961, acc 0.3007\n",
      "step  1000, loss 11.3820, acc 0.4244\n",
      "step  1500, loss 7.4669, acc 0.4967\n",
      "step  2000, loss 8.5507, acc 0.5779\n",
      "step  2500, loss 4.8071, acc 0.6389\n",
      "step  3000, loss 3.7524, acc 0.6766\n",
      "step  3500, loss 5.5406, acc 0.6988\n",
      "step  4000, loss 7.0105, acc 0.7137\n",
      "step  4500, loss 2.6580, acc 0.7296\n",
      "step  5000, loss 3.7784, acc 0.7383\n",
      "step  5500, loss 2.8094, acc 0.7453\n",
      "step  6000, loss 3.8556, acc 0.7472\n",
      "step  6500, loss 2.5606, acc 0.7522\n",
      "step  7000, loss 4.2431, acc 0.7616\n",
      "step  7500, loss 3.5099, acc 0.7596\n",
      "step  8000, loss 3.5808, acc 0.7660\n",
      "step  8500, loss 2.9482, acc 0.7662\n",
      "step  9000, loss 3.6419, acc 0.7706\n",
      "step  9500, loss 3.4500, acc 0.7744\n",
      "step 10000, loss 4.2020, acc 0.7751\n",
      "step 10500, loss 4.0295, acc 0.7824\n",
      "step 11000, loss 5.0582, acc 0.7824\n",
      "step 11500, loss 3.4962, acc 0.7795\n",
      "step 12000, loss 3.0215, acc 0.7853\n",
      "step 12500, loss 3.1658, acc 0.7877\n",
      "step 13000, loss 3.6675, acc 0.7839\n",
      "step 13500, loss 2.6624, acc 0.7898\n",
      "step 14000, loss 4.0289, acc 0.7903\n",
      "step 14500, loss 1.0002, acc 0.7893\n",
      "step 15000, loss 2.5184, acc 0.7908\n",
      "step 15500, loss 2.9239, acc 0.7908\n",
      "step 16000, loss 4.8027, acc 0.7923\n",
      "step 16500, loss 3.8363, acc 0.7939\n",
      "step 17000, loss 1.5303, acc 0.7954\n",
      "step 17500, loss 3.0221, acc 0.7972\n",
      "step 18000, loss 2.8393, acc 0.7972\n",
      "step 18500, loss 4.8626, acc 0.7966\n",
      "step 19000, loss 2.7451, acc 0.7953\n",
      "step 19500, loss 5.0369, acc 0.7995\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(labels * tf.log(output + 1e-7),axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 浮现上述代码。\n",
    "\n",
    "已完成，如上所示。但存在一个问题，代码显示数字一直是黑的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 计算模型参数。\n",
    "\n",
    "输入层28*28\n",
    "输出层10\n",
    "隐藏层128\n",
    "学习率0.01\n",
    "步长32\n",
    "训练20000步\n",
    "每500次保存一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. 使用不同大小的批次进行实验，观察模型收敛速度与收敛平稳性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "当batch_size从32变为64，则收敛速度加快，并且最终正确率得到提升，\n",
    "\n",
    "当batch_size从32变为16，则收敛速度变慢，并且最终正确率降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 13.7134, acc 0.1089\n",
      "step   500, loss 13.0960, acc 0.1642\n",
      "step  1000, loss 13.6415, acc 0.2189\n",
      "step  1500, loss 11.0697, acc 0.2617\n",
      "step  2000, loss 12.3430, acc 0.2757\n",
      "step  2500, loss 11.8710, acc 0.2909\n",
      "step  3000, loss 11.8445, acc 0.3313\n",
      "step  3500, loss 8.6527, acc 0.3878\n",
      "step  4000, loss 9.1477, acc 0.4037\n",
      "step  4500, loss 8.9540, acc 0.4158\n",
      "step  5000, loss 8.5739, acc 0.4222\n",
      "step  5500, loss 9.3395, acc 0.4248\n",
      "step  6000, loss 7.2846, acc 0.4338\n",
      "step  6500, loss 9.2170, acc 0.4694\n",
      "step  7000, loss 7.5167, acc 0.4977\n",
      "step  7500, loss 7.3035, acc 0.5046\n",
      "step  8000, loss 7.8265, acc 0.5158\n",
      "step  8500, loss 5.5371, acc 0.5178\n",
      "step  9000, loss 8.0591, acc 0.5250\n",
      "step  9500, loss 7.0517, acc 0.5210\n",
      "step 10000, loss 7.5554, acc 0.5295\n",
      "step 10500, loss 7.7089, acc 0.5303\n",
      "step 11000, loss 10.0715, acc 0.5278\n",
      "step 11500, loss 7.0687, acc 0.5373\n",
      "step 12000, loss 7.2326, acc 0.5303\n",
      "step 12500, loss 6.5623, acc 0.5355\n",
      "step 13000, loss 6.0389, acc 0.5361\n",
      "step 13500, loss 7.8389, acc 0.5405\n",
      "step 14000, loss 8.3399, acc 0.5367\n",
      "step 14500, loss 7.7294, acc 0.5367\n",
      "step 15000, loss 6.0443, acc 0.5425\n",
      "step 15500, loss 6.0443, acc 0.5439\n",
      "step 16000, loss 8.0602, acc 0.5404\n",
      "step 16500, loss 6.5480, acc 0.5437\n",
      "step 17000, loss 8.3109, acc 0.5410\n",
      "step 17500, loss 7.8072, acc 0.5446\n",
      "step 18000, loss 8.0811, acc 0.5424\n",
      "step 18500, loss 8.2798, acc 0.5435\n",
      "step 19000, loss 7.7827, acc 0.5462\n",
      "step 19500, loss 7.4678, acc 0.5440\n"
     ]
    }
   ],
   "source": [
    "#当batch_size从32变为64，则收敛速度加快，并且最终正确率得到提升\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(labels * tf.log(output + 1e-7),axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(64)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 64):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(64)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 12.0886, acc 0.0988\n",
      "step   500, loss 10.2320, acc 0.4400\n",
      "step  1000, loss 7.0589, acc 0.5485\n",
      "step  1500, loss 9.0664, acc 0.5911\n",
      "step  2000, loss 6.6772, acc 0.6086\n",
      "step  2500, loss 5.7152, acc 0.6262\n",
      "step  3000, loss 7.1697, acc 0.6204\n",
      "step  3500, loss 8.0590, acc 0.6373\n",
      "step  4000, loss 7.0317, acc 0.6405\n",
      "step  4500, loss 6.3222, acc 0.6422\n",
      "step  5000, loss 3.1286, acc 0.6543\n",
      "step  5500, loss 7.0517, acc 0.6638\n",
      "step  6000, loss 7.0517, acc 0.6847\n",
      "step  6500, loss 5.0342, acc 0.6888\n",
      "step  7000, loss 4.0294, acc 0.7003\n",
      "step  7500, loss 1.0074, acc 0.7076\n",
      "step  8000, loss 3.6345, acc 0.7119\n",
      "step  8500, loss 4.8239, acc 0.7119\n",
      "step  9000, loss 6.0434, acc 0.7157\n",
      "step  9500, loss 5.0369, acc 0.7134\n",
      "step 10000, loss 4.0295, acc 0.7189\n",
      "step 10500, loss 4.6433, acc 0.7272\n",
      "step 11000, loss 3.0266, acc 0.7255\n",
      "step 11500, loss 5.0510, acc 0.7297\n",
      "step 12000, loss 4.0485, acc 0.7313\n",
      "step 12500, loss 4.0306, acc 0.7166\n",
      "step 13000, loss 5.1369, acc 0.7326\n",
      "step 13500, loss 3.0222, acc 0.7354\n",
      "step 14000, loss 3.0221, acc 0.7331\n",
      "step 14500, loss 5.6101, acc 0.7344\n",
      "step 15000, loss 4.0295, acc 0.7351\n",
      "step 15500, loss 5.0369, acc 0.7351\n",
      "step 16000, loss 3.0221, acc 0.7426\n",
      "step 16500, loss 2.0148, acc 0.7333\n",
      "step 17000, loss 4.0296, acc 0.7390\n",
      "step 17500, loss 6.7567, acc 0.7411\n",
      "step 18000, loss 5.0369, acc 0.7413\n",
      "step 18500, loss 7.7224, acc 0.7393\n",
      "step 19000, loss 2.0148, acc 0.7363\n",
      "step 19500, loss 2.0148, acc 0.7429\n"
     ]
    }
   ],
   "source": [
    "#当batch_size从32变为16，则收敛速度变慢，并且最终正确率降低\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(labels * tf.log(output + 1e-7),axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(16)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 16):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(16)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. 使用不同大小的学习率进行实验，观察模型收敛速度与收敛平稳性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 15.2603, acc 0.1015\n",
      "step   500, loss 13.5911, acc 0.1322\n",
      "step  1000, loss 12.2339, acc 0.1702\n",
      "step  1500, loss 13.8662, acc 0.2010\n",
      "step  2000, loss 11.4590, acc 0.2373\n",
      "step  2500, loss 13.9788, acc 0.2515\n",
      "step  3000, loss 12.5850, acc 0.2781\n",
      "step  3500, loss 10.2408, acc 0.2959\n",
      "step  4000, loss 8.5683, acc 0.3192\n",
      "step  4500, loss 8.9627, acc 0.3402\n",
      "step  5000, loss 8.7279, acc 0.3606\n",
      "step  5500, loss 9.4678, acc 0.3746\n",
      "step  6000, loss 8.6496, acc 0.3921\n",
      "step  6500, loss 9.0665, acc 0.4087\n",
      "step  7000, loss 10.4447, acc 0.4182\n",
      "step  7500, loss 9.8031, acc 0.4291\n",
      "step  8000, loss 7.7719, acc 0.4425\n",
      "step  8500, loss 7.5303, acc 0.4534\n",
      "step  9000, loss 8.4727, acc 0.4681\n",
      "step  9500, loss 8.7967, acc 0.4682\n",
      "step 10000, loss 9.1070, acc 0.4784\n",
      "step 10500, loss 9.8013, acc 0.4908\n",
      "step 11000, loss 10.0700, acc 0.4958\n",
      "step 11500, loss 5.5186, acc 0.5066\n",
      "step 12000, loss 9.3205, acc 0.5220\n",
      "step 12500, loss 7.1234, acc 0.5234\n",
      "step 13000, loss 8.0605, acc 0.5421\n",
      "step 13500, loss 6.9614, acc 0.5493\n",
      "step 14000, loss 6.7289, acc 0.5577\n",
      "step 14500, loss 4.6959, acc 0.5721\n",
      "step 15000, loss 9.0664, acc 0.5718\n",
      "step 15500, loss 5.0381, acc 0.5813\n",
      "step 16000, loss 7.8980, acc 0.5887\n",
      "step 16500, loss 5.4551, acc 0.5974\n",
      "step 17000, loss 6.4075, acc 0.6009\n",
      "step 17500, loss 5.0677, acc 0.6058\n",
      "step 18000, loss 5.4285, acc 0.6082\n",
      "step 18500, loss 6.5346, acc 0.6109\n",
      "step 19000, loss 5.1391, acc 0.6191\n",
      "step 19500, loss 3.1060, acc 0.6207\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(labels * tf.log(output + 1e-7),axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 14.5800, acc 0.0913\n",
      "step   500, loss 10.0645, acc 0.5212\n",
      "step  1000, loss 4.3951, acc 0.7730\n",
      "step  1500, loss 1.7427, acc 0.8539\n",
      "step  2000, loss 1.0074, acc 0.8736\n",
      "step  2500, loss 0.9576, acc 0.8944\n",
      "step  3000, loss 1.0984, acc 0.8999\n",
      "step  3500, loss 1.2778, acc 0.8987\n",
      "step  4000, loss 1.0085, acc 0.9042\n",
      "step  4500, loss 1.5085, acc 0.9176\n",
      "step  5000, loss 2.1303, acc 0.9223\n",
      "step  5500, loss 1.2409, acc 0.9173\n",
      "step  6000, loss 1.9968, acc 0.9217\n",
      "step  6500, loss 0.0588, acc 0.9296\n",
      "step  7000, loss 1.0198, acc 0.9228\n",
      "step  7500, loss 0.0000, acc 0.9309\n",
      "step  8000, loss 0.0240, acc 0.9326\n",
      "step  8500, loss 1.0949, acc 0.9304\n",
      "step  9000, loss 1.0069, acc 0.9369\n",
      "step  9500, loss 0.6307, acc 0.9343\n",
      "step 10000, loss 0.5037, acc 0.9339\n",
      "step 10500, loss -0.0000, acc 0.9381\n",
      "step 11000, loss -0.0000, acc 0.9394\n",
      "step 11500, loss 1.4347, acc 0.9383\n",
      "step 12000, loss 0.2074, acc 0.9380\n",
      "step 12500, loss 0.9790, acc 0.9441\n",
      "step 13000, loss 0.6573, acc 0.9368\n",
      "step 13500, loss 1.4214, acc 0.9456\n",
      "step 14000, loss 0.2175, acc 0.9436\n",
      "step 14500, loss 0.5546, acc 0.9415\n",
      "step 15000, loss 0.0257, acc 0.9449\n",
      "step 15500, loss 0.0299, acc 0.9477\n",
      "step 16000, loss -0.0000, acc 0.9473\n",
      "step 16500, loss 0.2870, acc 0.9458\n",
      "step 17000, loss 0.0000, acc 0.9488\n",
      "step 17500, loss 1.0091, acc 0.9475\n",
      "step 18000, loss 0.5037, acc 0.9464\n",
      "step 18500, loss 1.7761, acc 0.9523\n",
      "step 19000, loss 0.0267, acc 0.9497\n",
      "step 19500, loss 0.0000, acc 0.9478\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(labels * tf.log(output + 1e-7),axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "上面的学习率分别为0.1和0.001，学习率调高，会使训练速度加快，但不稳；调小学习率，会使训练变慢，模型平稳性更好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. 使用不同的激活函数进行实验，观察模型收敛速度与收敛平性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 13.0959, acc 0.1292\n",
      "step   500, loss 12.1683, acc 0.2962\n",
      "step  1000, loss 12.5876, acc 0.3827\n",
      "step  1500, loss 10.8858, acc 0.4551\n",
      "step  2000, loss 8.8246, acc 0.4865\n",
      "step  2500, loss 8.5445, acc 0.5270\n",
      "step  3000, loss 7.5542, acc 0.5673\n",
      "step  3500, loss 6.9494, acc 0.5943\n",
      "step  4000, loss 5.5768, acc 0.6185\n",
      "step  4500, loss 4.7582, acc 0.6385\n",
      "step  5000, loss 4.3366, acc 0.6432\n",
      "step  5500, loss 4.5212, acc 0.6510\n",
      "step  6000, loss 4.7848, acc 0.6564\n",
      "step  6500, loss 5.1646, acc 0.6658\n",
      "step  7000, loss 6.5453, acc 0.6729\n",
      "step  7500, loss 5.1383, acc 0.7100\n",
      "step  8000, loss 4.9105, acc 0.7373\n",
      "step  8500, loss 4.3792, acc 0.7497\n",
      "step  9000, loss 3.6290, acc 0.7583\n",
      "step  9500, loss 3.1404, acc 0.7621\n",
      "step 10000, loss 3.9865, acc 0.7626\n",
      "step 10500, loss 2.5185, acc 0.7674\n",
      "step 11000, loss 5.0380, acc 0.7740\n",
      "step 11500, loss 3.0797, acc 0.7700\n",
      "step 12000, loss 1.5292, acc 0.7762\n",
      "step 12500, loss 3.6769, acc 0.7777\n",
      "step 13000, loss 4.0816, acc 0.7791\n",
      "step 13500, loss 5.8161, acc 0.7806\n",
      "step 14000, loss 3.0073, acc 0.7839\n",
      "step 14500, loss 2.5185, acc 0.7847\n",
      "step 15000, loss 4.0584, acc 0.7861\n",
      "step 15500, loss 2.0148, acc 0.7860\n",
      "step 16000, loss 2.0148, acc 0.7933\n",
      "step 16500, loss 4.0869, acc 0.7842\n",
      "step 17000, loss 1.6859, acc 0.7944\n",
      "step 17500, loss 3.7119, acc 0.7952\n",
      "step 18000, loss 2.5185, acc 0.7908\n",
      "step 18500, loss 3.4646, acc 0.7926\n",
      "step 19000, loss 4.0566, acc 0.7917\n",
      "step 19500, loss 2.7010, acc 0.7970\n"
     ]
    }
   ],
   "source": [
    "#激活函数  relu\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.relu(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 11.6617, acc 0.0825\n",
      "step   500, loss 7.9587, acc 0.2768\n",
      "step  1000, loss 6.2741, acc 0.4131\n",
      "step  1500, loss 4.2745, acc 0.5046\n",
      "step  2000, loss 2.7654, acc 0.6253\n",
      "step  2500, loss 1.1400, acc 0.6733\n",
      "step  3000, loss 2.3333, acc 0.7118\n",
      "step  3500, loss 1.9026, acc 0.7305\n",
      "step  4000, loss 2.3149, acc 0.7539\n",
      "step  4500, loss 1.5815, acc 0.7606\n",
      "step  5000, loss 1.6918, acc 0.7745\n",
      "step  5500, loss 2.2712, acc 0.7843\n",
      "step  6000, loss 1.2548, acc 0.7916\n",
      "step  6500, loss 0.8253, acc 0.7975\n",
      "step  7000, loss 0.6048, acc 0.8034\n",
      "step  7500, loss 1.6530, acc 0.8067\n",
      "step  8000, loss 1.2453, acc 0.8126\n",
      "step  8500, loss 1.1172, acc 0.8191\n",
      "step  9000, loss 0.4795, acc 0.8246\n",
      "step  9500, loss 0.4343, acc 0.8210\n",
      "step 10000, loss 0.7750, acc 0.8281\n",
      "step 10500, loss 0.6323, acc 0.8323\n",
      "step 11000, loss 0.9584, acc 0.8356\n",
      "step 11500, loss 1.3960, acc 0.8379\n",
      "step 12000, loss 0.5025, acc 0.8407\n",
      "step 12500, loss 1.0205, acc 0.8430\n",
      "step 13000, loss 0.5352, acc 0.8490\n",
      "step 13500, loss 0.2568, acc 0.8415\n",
      "step 14000, loss 1.4848, acc 0.8488\n",
      "step 14500, loss 1.1452, acc 0.8477\n",
      "step 15000, loss 0.0936, acc 0.8573\n",
      "step 15500, loss 1.3378, acc 0.8494\n",
      "step 16000, loss 0.9362, acc 0.8519\n",
      "step 16500, loss 0.5281, acc 0.8581\n",
      "step 17000, loss 0.4147, acc 0.8574\n",
      "step 17500, loss 1.2445, acc 0.8569\n",
      "step 18000, loss 0.5499, acc 0.8608\n",
      "step 18500, loss 0.4526, acc 0.8595\n",
      "step 19000, loss 0.5420, acc 0.8603\n",
      "step 19500, loss 1.2127, acc 0.8649\n"
     ]
    }
   ],
   "source": [
    "#激活函数  tf.nn.tanh\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.tanh(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_normal([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "使用relu和tanh激活函数，tanh激活函数的正确率较高、收敛速度较快。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. 尝试使用不同的参数初始化方法（如使用不同的正态分布、均匀分布、固定值等方法），观察模型收敛速度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 5.7012, acc 0.1122\n",
      "step   500, loss 1.6599, acc 0.6014\n",
      "step  1000, loss 0.5949, acc 0.7069\n",
      "step  1500, loss 0.7479, acc 0.7534\n",
      "step  2000, loss 0.4924, acc 0.7806\n",
      "step  2500, loss 0.3338, acc 0.7997\n",
      "step  3000, loss 0.4499, acc 0.8180\n",
      "step  3500, loss 0.5554, acc 0.8229\n",
      "step  4000, loss 0.2113, acc 0.8324\n",
      "step  4500, loss 0.5579, acc 0.8355\n",
      "step  5000, loss 0.3798, acc 0.8399\n",
      "step  5500, loss 0.3895, acc 0.8457\n",
      "step  6000, loss 0.4924, acc 0.8530\n",
      "step  6500, loss 0.6961, acc 0.8560\n",
      "step  7000, loss 0.6093, acc 0.8562\n",
      "step  7500, loss 0.1468, acc 0.8508\n",
      "step  8000, loss 0.4930, acc 0.8618\n",
      "step  8500, loss 0.2659, acc 0.8667\n",
      "step  9000, loss 0.2459, acc 0.8657\n",
      "step  9500, loss 0.3815, acc 0.8652\n",
      "step 10000, loss 0.4989, acc 0.8697\n",
      "step 10500, loss 0.6632, acc 0.8743\n",
      "step 11000, loss 0.5390, acc 0.8686\n",
      "step 11500, loss 0.4561, acc 0.8751\n",
      "step 12000, loss 0.2772, acc 0.8690\n",
      "step 12500, loss 0.4273, acc 0.8766\n",
      "step 13000, loss 0.5633, acc 0.8780\n",
      "step 13500, loss 0.4210, acc 0.8762\n",
      "step 14000, loss 0.3594, acc 0.8829\n",
      "step 14500, loss 0.5943, acc 0.8801\n",
      "step 15000, loss 0.3951, acc 0.8802\n",
      "step 15500, loss 0.1346, acc 0.8824\n",
      "step 16000, loss 0.2520, acc 0.8840\n",
      "step 16500, loss 0.2752, acc 0.8834\n",
      "step 17000, loss 0.4851, acc 0.8830\n",
      "step 17500, loss 0.3871, acc 0.8810\n",
      "step 18000, loss 0.2846, acc 0.8869\n",
      "step 18500, loss 0.2049, acc 0.8836\n",
      "step 19000, loss 0.5337, acc 0.8857\n",
      "step 19500, loss 0.5100, acc 0.8937\n"
     ]
    }
   ],
   "source": [
    "#初始化方法：tf.random_uniform\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    # 输入、标记占位符\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # 创建128个隐藏层神经元参数\n",
    "    hidden_weight = tf.Variable(tf.random_normal([784, 128]), name='hidden_weight')\n",
    "    hidden_bias = tf.Variable(tf.zeros([128, ]), name='hidden_bias')\n",
    "    # 隐藏层前向传播\n",
    "    hidden_output = tf.nn.tanh(tf.matmul(inputs, hidden_weight) + hidden_bias)\n",
    "    \n",
    "    \n",
    "    # 创建输出层10个神经元参数\n",
    "    output_weight = tf.Variable(tf.random_uniform([128, 10], name='output_weight'))\n",
    "    output_bias = tf.Variable(tf.zeros([10, ]), name='output_bias')\n",
    "    # 输出层前向传播\n",
    "    logits = tf.matmul(hidden_output, output_weight) + output_bias\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    \n",
    "    # 代价函数\n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "        labels * tf.log(output + 1e-7),\n",
    "        axis=1))\n",
    "    \n",
    "    \n",
    "    # 正确率\n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)),\n",
    "                tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # 定义梯度下降法优化器\n",
    "    optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optim.minimize(loss)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 训练模型\n",
    "    for step in range(20000):\n",
    "        batch_images, batch_labels = mnist.train.next_batch(32)\n",
    "        res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "            inputs: batch_images,\n",
    "            labels: batch_labels\n",
    "        })\n",
    "        \n",
    "        # 输出代价并验证模型\n",
    "        if step % 500 == 0:\n",
    "            accs = []\n",
    "            for test_step in range(10000 // 32):\n",
    "                batch_images, batch_labels = mnist.test.next_batch(32)\n",
    "                res_acc = sess.run(acc, feed_dict={\n",
    "                    inputs: batch_images,\n",
    "                    labels: batch_labels\n",
    "                })\n",
    "                accs.append(res_acc)\n",
    "            accs = np.mean(accs)\n",
    "            print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用tf.random_uniform比tf.random_normal的正确率高，且收敛效果更平稳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. 思考如何改进模型以使得模型性能增强。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以调整模型的学习率、训练批次的大小、激活函数、增加隐藏层、增加训练总的步数，初始化方法，改变不同的组合方式来查看是否会提升模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 思考如何给模型添加新的隐藏层并进行实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两层隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step     0, loss 14.3552, acc 0.1057\n",
      "step  1000, loss 13.5618, acc 0.1995\n",
      "step  2000, loss 13.0960, acc 0.2320\n",
      "step  3000, loss 11.3413, acc 0.3091\n",
      "step  4000, loss 9.0664, acc 0.3439\n",
      "step  5000, loss 11.0812, acc 0.4153\n",
      "step  6000, loss 9.1130, acc 0.4266\n",
      "step  7000, loss 9.0664, acc 0.4344\n",
      "step  8000, loss 8.3109, acc 0.4273\n",
      "step  9000, loss 10.3443, acc 0.4349\n",
      "step 10000, loss 8.5627, acc 0.4436\n",
      "step 11000, loss 8.0590, acc 0.4461\n",
      "step 12000, loss 9.5701, acc 0.4397\n",
      "step 13000, loss 9.3183, acc 0.4444\n",
      "step 14000, loss 9.3183, acc 0.4497\n",
      "step 15000, loss 8.0590, acc 0.4508\n",
      "step 16000, loss 10.0738, acc 0.4444\n",
      "step 17000, loss 7.8072, acc 0.4556\n",
      "step 18000, loss 8.5627, acc 0.4525\n",
      "step 19000, loss 11.8367, acc 0.4477\n",
      "step 20000, loss 10.5772, acc 0.4527\n",
      "step 21000, loss 8.5627, acc 0.4510\n",
      "step 22000, loss 9.5701, acc 0.4514\n",
      "step 23000, loss 9.3183, acc 0.4557\n",
      "step 24000, loss 8.8146, acc 0.4597\n",
      "step 25000, loss 9.3183, acc 0.4496\n",
      "step 26000, loss 9.3183, acc 0.4627\n",
      "step 27000, loss 8.8146, acc 0.4566\n",
      "step 28000, loss 8.3109, acc 0.4564\n",
      "step 29000, loss 8.4174, acc 0.4596\n",
      "step 30000, loss 7.5554, acc 0.4569\n",
      "step 31000, loss 10.3257, acc 0.4594\n",
      "step 32000, loss 8.3109, acc 0.4618\n",
      "step 33000, loss 9.2362, acc 0.4635\n",
      "step 34000, loss 7.8072, acc 0.4582\n",
      "step 35000, loss 6.7998, acc 0.4621\n",
      "step 36000, loss 8.5627, acc 0.4598\n",
      "step 37000, loss 8.8146, acc 0.4591\n",
      "step 38000, loss 9.3183, acc 0.4658\n",
      "step 39000, loss 9.0664, acc 0.4599\n",
      "step 40000, loss 7.6658, acc 0.4625\n",
      "step 41000, loss 8.0590, acc 0.4622\n",
      "step 42000, loss 6.7998, acc 0.4593\n",
      "step 43000, loss 7.5554, acc 0.4638\n",
      "step 44000, loss 7.5554, acc 0.4631\n",
      "step 45000, loss 8.3109, acc 0.4628\n",
      "step 46000, loss 7.5554, acc 0.4607\n",
      "step 47000, loss 9.3183, acc 0.4652\n",
      "step 48000, loss 8.3108, acc 0.4684\n",
      "step 49000, loss 9.0405, acc 0.4650\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    inputs = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    \n",
    "    h1_w = tf.Variable(tf.random_normal([784, 64]), name='h1_w')\n",
    "    h1_b = tf.Variable(tf.zeros([64, ]), name='h1_b')\n",
    "    h1_o = tf.nn.relu(tf.matmul(inputs, h1_w)+h1_b)\n",
    "    \n",
    "    h2_w = tf.Variable(tf.random_normal([64, 128]), name='h2_w')\n",
    "    h2_b = tf.Variable(tf.zeros([128, ]), name='h2_b')\n",
    "    h2_o = tf.nn.relu(tf.matmul(h1_o, h2_w)+h2_b)\n",
    "    \n",
    "    o_w = tf.Variable(tf.random_normal([128, 10]), name='o_w')\n",
    "    o_b = tf.Variable(tf.zeros([10, ]), name='o_w')\n",
    "    \n",
    "    logits = tf.matmul(h2_o, o_w) + o_b\n",
    "    output = tf.nn.softmax(logits)\n",
    "    \n",
    "    loss = tf.reduce_mean(-1 * tf.reduce_sum(\n",
    "            labels * tf.log(output + 1e-7), axis=1))\n",
    "    \n",
    "    acc = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(output, axis=1)), \n",
    "                dtype=tf.float32))\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        optim = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        \n",
    "        train_op = optim.minimize(loss)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(50000):\n",
    "            batch_images, batch_labels = mnist.train.next_batch(64)\n",
    "            res_loss, _ = sess.run([loss, train_op], feed_dict={\n",
    "                        inputs: batch_images,\n",
    "                        labels: batch_labels\n",
    "            })\n",
    "            \n",
    "            if step % 1000 ==0:\n",
    "                accs = []\n",
    "                for test_step in range(10000 // 64):\n",
    "                    batch_images, batch_labels = mnist.test.next_batch(64)\n",
    "                    \n",
    "                    res_acc = sess.run(acc, feed_dict={\n",
    "                        inputs: batch_images,\n",
    "                        labels: batch_labels\n",
    "                    })\n",
    "                    \n",
    "                    accs.append(res_acc)\n",
    "                accs = np.mean(accs)\n",
    "                print('step %5d, loss %2.4f, acc %.4f' % (step, res_loss, accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
